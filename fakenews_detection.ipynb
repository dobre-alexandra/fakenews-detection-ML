{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dbbfe92",
   "metadata": {},
   "source": [
    "# Part 2: Fake News with Spark MLib \n",
    "\n",
    "* My question: How can machine learning detect fake news from social media posts?\n",
    "* Dataset: LIAR dataset, Fake News Kaggle dataset, or Twitter fake news dataset\n",
    "* Pipeline Components:\n",
    "    * Text Preprocessing: Tokenization, Stopword Removal, TF-IDF embeddings (Transformer)\n",
    "    * Feature Engineering: Extract metadata features (source credibility, number of retweets, etc.) (Transformer)\n",
    "    * Fake News Classifier: Train a machine learning model (Logistic Regression, Random Forest, or Neural Networks) (Estimator)\n",
    "    * Evaluation: Precision, recall, and AUC-ROC for classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09629f11",
   "metadata": {},
   "source": [
    "## Import packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352eee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import \n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import gc\n",
    "\n",
    "# Parse the data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, count, round, lit, explode, col, trim\n",
    "from pyspark.sql.functions import col, sum as spark_sum, isnan, when\n",
    "from pyspark.sql.functions import col, when, count, lit, round as spark_round\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f063f42",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "\n",
    "This project uses a [Fake News Detection Dataset](https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets) from Kaggle, which consists of two CSV files:  \n",
    "- `True.csv`: Contains real news articles sourced from [Reuters](https://www.reuters.com/).\n",
    "- `Fake.csv`: Contains fake news articles collected from unreliable sources flagged by **PolitiFact** and **Wikipedia**.\n",
    "\n",
    "Each article includes:\n",
    "- `title`: Headline of the article  \n",
    "- `text`: Main body of the article  \n",
    "- `subject`: General topic/category (e.g., politics, world)  \n",
    "- `date`: Publication date  \n",
    "\n",
    "The dataset covers primarily political and world news stories, with articles mainly from 2016â€“2017. \n",
    "\n",
    "\n",
    "### Import \n",
    "Note: You need to upload `True.csv` and `False.csv` to the GCS bucket we created at the beginning: [st445-w09-ad](https://console.cloud.google.com/storage/browser/st446-w09-ad;tab=objects?forceOnBucketsSortingFiltering=true&cloudshell=true&inv=1&invt=Abtldw&project=st446-wt2025-id&prefix=&forceOnObjectsSortingFiltering=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c496836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"subject\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# read in the 2 datasets \n",
    "fake_df = spark.read.csv(\"gs://st446-w09-ad/Fake.csv\", header=True, schema=schema)\n",
    "true_df = spark.read.csv(\"gs://st446-w09-ad/True.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1532e6",
   "metadata": {},
   "source": [
    "### Quick cleaning and summary statistics\n",
    "\n",
    "#### Append fake data to true data and create a binary indictor `fake`, which =1 if the row comes from `Fake.csv`, and =0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3c338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+------------------+----+---+\n",
      "|               title|                text|     subject|              date|fake| id|\n",
      "+--------------------+--------------------+------------+------------------+----+---+\n",
      "|As U.S. budget fi...|WASHINGTON (Reute...|politicsNews|December 31, 2017 |   0|  0|\n",
      "|U.S. military to ...|WASHINGTON (Reute...|politicsNews|December 29, 2017 |   0|  1|\n",
      "|Senior U.S. Repub...|WASHINGTON (Reute...|politicsNews|December 31, 2017 |   0|  2|\n",
      "|FBI Russia probe ...|WASHINGTON (Reute...|politicsNews|December 30, 2017 |   0|  3|\n",
      "|Trump wants Posta...|SEATTLE/WASHINGTO...|politicsNews|December 29, 2017 |   0|  4|\n",
      "+--------------------+--------------------+------------+------------------+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add label column: 1 for fake, 0 for true\n",
    "fake_df = fake_df.withColumn(\"fake\", F.lit(1))\n",
    "true_df = true_df.withColumn(\"fake\", F.lit(0))\n",
    "\n",
    "# Combine the datasets\n",
    "news_df = true_df.unionByName(fake_df)\n",
    "\n",
    "# Add id col, this will be useful later in ML pipeline\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "news_df = news_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Preview combined data\n",
    "news_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dcfa74",
   "metadata": {},
   "source": [
    "**Let's unpersist the initial dataframes so we can preserve memory for later operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7cf510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpersist dfs and clear cache\n",
    "fake_df.unpersist(blocking=True)\n",
    "true_df.unpersist(blocking=True)\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# garbage collection\n",
    "del fake_df\n",
    "del true_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02142b54",
   "metadata": {},
   "source": [
    "#### Let's create a breakdown of fake vs. real news articles in `news_df`. \n",
    "After loading and labeling the data (e.g., assigning `fake = 1` for fake news and `fake = 0` for real news), we compute the total number of observations and calculate the percentage of each label. About 48% of our data include real news, and 52% of the data are fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870a9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+\n",
      "|fake|count|percent|\n",
      "+----+-----+-------+\n",
      "|   0|21417|  47.69|\n",
      "|   1|23489|  52.31|\n",
      "+----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# cache news_df because it's large and will use it later\n",
    "news_df.cache()\n",
    "\n",
    "# count total number of rows/materialize cache\n",
    "total_count = news_df.count()\n",
    "\n",
    "# group by label and count\n",
    "breakdown_df = news_df.groupBy(\"fake\").agg(\n",
    "    count(\"*\").alias(\"count\")\n",
    ").withColumn(\n",
    "    \"percent\", round((col(\"count\") / total_count) * 100, 2)\n",
    ")\n",
    "\n",
    "breakdown_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465a430",
   "metadata": {},
   "source": [
    "**Analysis of missing data**\n",
    "\n",
    "Our model will rely heavily on article text, so it's important to understand the extent of missing data for the column. We will do the following: \n",
    "\n",
    "1. **Create a missingness indicator**:\n",
    "   A new column `is_missing` is added, marking rows as `True` if the `text` field is either null or an empty string (after trimming whitespace).\n",
    "\n",
    "2. **Summarize missing data by label**:\n",
    "   The data is grouped by `is_missing`, and counts of real (`fake = 0`) and fake (`fake = 1`) articles are computed. Percentages within each group are also calculated to assess whether missing data is more common in either class.\n",
    "\n",
    "3. **Compute overall missingness**:\n",
    "   The percentage of all rows that have missing `text` is calculated and printed.\n",
    "\n",
    "4. **Drop missing entries**:\n",
    "   Rows where `is_missing` is `True` are dropped from the dataset, and the number of dropped rows is reported.\n",
    "\n",
    "We find that there are 639 observations where information is missing in `text`. 99.9% of the missing observations come from our fake news dataset. Given that only about 1.4% of our data is missing `text` info, we will just drop them so we don't run into issues later tokenizing/filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d8bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----+------------+------------+\n",
      "|is_missing|real_count|fake_count|total|percent_real|percent_fake|\n",
      "+----------+----------+----------+-----+------------+------------+\n",
      "|false     |21416     |22851     |44267|48.38       |51.62       |\n",
      "|true      |1         |638       |639  |0.16        |99.84       |\n",
      "+----------+----------+----------+-----+------------+------------+\n",
      "\n",
      "Missing text in 639 out of 44906 rows (1.42%)\n",
      "Dropping 639 rows...\n"
     ]
    }
   ],
   "source": [
    "# Create is_missing column based on null or empty text\n",
    "news_df = news_df.withColumn(\"is_missing\", (col(\"text\").isNull()) | (trim(col(\"text\")) == \"\"))\n",
    "\n",
    "# Aggregate real/fake counts by is_missing\n",
    "grouped = news_df.groupBy(\"is_missing\") \\\n",
    "    .agg(\n",
    "        count(when(col(\"fake\") == 0, True)).alias(\"real_count\"),\n",
    "        count(when(col(\"fake\") == 1, True)).alias(\"fake_count\"),\n",
    "        count(\"*\").alias(\"total\")\n",
    "    ) \\\n",
    "    .withColumn(\"percent_real\", spark_round((col(\"real_count\") / col(\"total\")) * 100, 2)) \\\n",
    "    .withColumn(\"percent_fake\", spark_round((col(\"fake_count\") / col(\"total\")) * 100, 2)) \\\n",
    "    .orderBy(\"is_missing\")\n",
    "\n",
    "# Show summary\n",
    "grouped.show(truncate=False)\n",
    "\n",
    "# Count total and missing rows\n",
    "total_rows = news_df.count()\n",
    "missing_rows = news_df.filter(col(\"is_missing\") == 1).count()\n",
    "\n",
    "# Use built-in round to avoid PySpark conflict\n",
    "missing_pct = builtins.round((missing_rows / total_rows) * 100, 2)\n",
    "\n",
    "print(f\"Missing text in {missing_rows} out of {total_rows} rows ({missing_pct}%)\")\n",
    "\n",
    "# Drop rows with missing text\n",
    "print(f\"Dropping {missing_rows} rows...\")\n",
    "news_df = news_df.filter(col(\"is_missing\") == 0).drop(\"is_missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295670e",
   "metadata": {},
   "source": [
    "### Pipeline Part 1: Text preprocessing\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "This block performs text preprocessing on the `text` column of the dataset. It includes the following steps:\n",
    "\n",
    "1. **Custom Stopword List**  \n",
    "   A custom list of stopwords is created by extending the default English stopwords provided by PySpark's `StopWordsRemover`. Additional terms are added to filter out vague adjectives, common filler verbs, source references (e.g., \"reuters\", \"getty\"), and other frequent but uninformative tokens found in news articles.\n",
    "\n",
    "2. **Tokenization**  \n",
    "   The `RegexTokenizer` splits each article's `text` column into individual word tokens. It splits on any non-word character (`\\\\W`), converts all tokens to lowercase, and filters out short tokens (with fewer than 2 characters).\n",
    "\n",
    "3. **Stopword Removal**  \n",
    "   The `StopWordsRemover` removes all tokens present in the custom stopword list. The result is a new column called `filtered` containing the cleaned list of tokens for each article.\n",
    "\n",
    "4. **Token Frequency Analysis**  \n",
    "   - The tokenized and filtered tokens are exploded so that each token appears in its own row.\n",
    "   - A count is computed for each unique token across the dataset.\n",
    "   - The resulting `token_counts` DataFrame is cached and materialized to improve performance for any subsequent operations.\n",
    "   - The top 50 most frequent tokens (after filtering) are displayed to understand common vocabulary patterns.\n",
    "\n",
    "This process prepares the data for downstream text feature extraction methods, such as TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65fe6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "# extend default English stopwords with my custom additions\n",
    "custom_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\") + [\n",
    "    \"\",  # empty string\n",
    "    \"new\", \"one\", \"two\", \"first\", \"last\", \"many\",  # vague adjectives/numbers\n",
    "    \"reuters\", \"(reuters)\",  # source tags\n",
    "    \"even\", \"may\", \"made\", \"make\", \"since\",\"like\",  # filler or common verbs\n",
    "    \"still\", \"go\", \"get\", \"take\", \"know\", \"see\", # high freq modifiers \n",
    "    \"said\", \"said.\", \"say\", \"saying\", \"says\", \"said,\",  # overused reporting verb\n",
    "    \"-\", \"featured\", \"getty\", \"image\", \"flickr\", \"subscribe\", \"filessupport\",\n",
    "    \"youtube\", \"images\", \"read\", \"via\"\n",
    "]\n",
    "\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"text\",\n",
    "    outputCol=\"tokens\",\n",
    "    pattern=\"\\\\W\",  # split on anything not a word char\n",
    "    minTokenLength=2,  # drop tiny tokens like \"a\", \"i\", \"â€”\"\n",
    "    toLowercase=True\n",
    ")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", \n",
    "                           outputCol=\"filtered\", \n",
    "                           stopWords=custom_stopwords)\n",
    "\n",
    "# Apply transformations\n",
    "tokenized_df = tokenizer.transform(news_df)\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "# Keep needed columns for next steps\n",
    "filtered_df = filtered_df.select(\"id\", \"title\", \"text\", \"fake\", \"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a68a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:====================================================> (164 + 4) / 170]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|token         |count |\n",
      "+--------------+------+\n",
      "|trump         |131192|\n",
      "|president     |54989 |\n",
      "|people        |41079 |\n",
      "|state         |34177 |\n",
      "|also          |30891 |\n",
      "|clinton       |28154 |\n",
      "|government    |27840 |\n",
      "|obama         |27688 |\n",
      "|donald        |27590 |\n",
      "|house         |27396 |\n",
      "|states        |26614 |\n",
      "|republican    |25340 |\n",
      "|year          |24732 |\n",
      "|united        |23394 |\n",
      "|told          |23099 |\n",
      "|white         |22322 |\n",
      "|campaign      |21408 |\n",
      "|election      |20962 |\n",
      "|time          |20861 |\n",
      "|party         |20454 |\n",
      "|news          |20178 |\n",
      "|washington    |18534 |\n",
      "|country       |17861 |\n",
      "|former        |17699 |\n",
      "|us            |16764 |\n",
      "|hillary       |16470 |\n",
      "|years         |16462 |\n",
      "|re            |16102 |\n",
      "|american      |15938 |\n",
      "|media         |15828 |\n",
      "|security      |15823 |\n",
      "|law           |15589 |\n",
      "|national      |15286 |\n",
      "|political     |15191 |\n",
      "|police        |14910 |\n",
      "|court         |14725 |\n",
      "|right         |14529 |\n",
      "|percent       |14341 |\n",
      "|according     |14049 |\n",
      "|republicans   |14020 |\n",
      "|going         |13912 |\n",
      "|administration|13839 |\n",
      "|russia        |13772 |\n",
      "|back          |13662 |\n",
      "|presidential  |13642 |\n",
      "|america       |13388 |\n",
      "|week          |13250 |\n",
      "|bill          |13201 |\n",
      "|democratic    |13166 |\n",
      "|called        |13040 |\n",
      "+--------------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Explode the list of tokens into individual rows\n",
    "tokens_df = filtered_df.select(explode(col(\"filtered\")).alias(\"token\"))\n",
    "\n",
    "# Group by token and count\n",
    "token_counts = tokens_df.groupBy(\"token\").count().orderBy(col(\"count\").desc())\n",
    "token_counts.cache()\n",
    "token_counts.count()  # materialize the cache\n",
    "\n",
    "# Show top N tokens\n",
    "token_counts.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43bd6b6",
   "metadata": {},
   "source": [
    "#### TF-IDF embeddings (Transformer)\n",
    "\n",
    "**Before we start, I'm going to clear unused DataFrames and caches to reduce the likelihood of Spark JVM or memory issues**\n",
    "\n",
    "We'll keep `news_df` and also `filtered_df`, which includes the final preprocessed text used for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3134762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44267"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df_name in [\n",
    "    \"tokenized_df\", \"tokens_df\", \n",
    "    \"token_counts\", \"breakdown_df\"\n",
    "]:\n",
    "    if df_name in globals():\n",
    "        try:\n",
    "            globals()[df_name].unpersist(blocking=True)\n",
    "        except:\n",
    "            pass  # not a Spark DF or not cached\n",
    "        del globals()[df_name]\n",
    "\n",
    "# Also clear Spark's catalog and Python memory\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# cache filtered_df\n",
    "filtered_df = filtered_df.cache()\n",
    "filtered_df.count()  # force materialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c15aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pausing to let Spark free up memory...\n",
      "Done waiting!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Wait 30 seconds to let Spark settle\n",
    "print(\"Pausing to let Spark free up memory...\")\n",
    "\n",
    "time.sleep(30)\n",
    "print(\"Done waiting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e988b468",
   "metadata": {},
   "source": [
    "Here, we transform the cleaned token data into numerical feature vectors using term frequency and TF-IDF methods:\n",
    "\n",
    "1. **CountVectorizer (Term Frequency)**  \n",
    "   - A `CountVectorizer` is initialized to convert the list of tokens in the `filtered` column into sparse term frequency vectors.\n",
    "   - `vocabSize=5000` limits the vocabulary to the top 5,000 most frequent tokens across the dataset. I started off with `vocabSize=10000`, but ran into memory constraints, so I was required to lower it.\n",
    "   - The output is stored in a new column called `raw_features`, which represents the raw count of each token per article.\n",
    "\n",
    "2. **Model Fitting**  \n",
    "   The vectorizer is fit to the `filtered_df` DataFrame, learning the vocabulary based on token frequency.\n",
    "\n",
    "3. **Transformation**  \n",
    "   The trained vectorizer model is used to transform the tokenized text into term frequency vectors. The resulting `vectorized_df` includes the original metadata along with the `raw_features` column.\n",
    "\n",
    "4. **Caching and Materialization**  \n",
    "   The resulting DataFrame is cached to optimize performance for future transformations. The `.count()` call forces materialization, ensuring that the computation is executed immediately and stored in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5074b68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# vectorizer \n",
    "vectorizer = CountVectorizer(inputCol=\"filtered\", \n",
    "                             outputCol=\"raw_features\", \n",
    "                             vocabSize=5000)\n",
    "\n",
    "# vectorizer models\n",
    "vectorizer_model = vectorizer.fit(filtered_df)\n",
    "vectorized_df = vectorizer_model.transform(filtered_df).select(\"id\", \"title\", \"text\", \"fake\", \"raw_features\").cache()\n",
    "vectorized_df.count()\n",
    "\n",
    "# unpersist filtered df\n",
    "filtered_df.unpersist(blocking=True)\n",
    "del filtered_df\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c8da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in idf_df =  44267\n",
      "Number of null features in idf_df =  0\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(vectorized_df)\n",
    "idf_df = idf_model.transform(vectorized_df).select(\"id\", \"title\", \"text\", \"fake\", \"features\").cache()\n",
    "\n",
    "print(\"Number of observations in idf_df = \", idf_df.count())\n",
    "print(\"Number of null features in idf_df = \", idf_df.filter(F.col(\"features\").isNull()).count())\n",
    "\n",
    "# drop vectorized df\n",
    "vectorized_df.unpersist(blocking=True)\n",
    "del vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "186c67cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1779"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.clearCache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a11f0",
   "metadata": {},
   "source": [
    "This block completes the TF-IDF transformation by applying the inverse document frequency (IDF) weighting to the term frequency vectors generated earlier. The final `features` column serves as the primary input for machine learning models, capturing the importance of each token within and across documents.\n",
    "\n",
    "1. **Initialize IDF Transformer**  \n",
    "   An `IDF` transformer is created to scale down the impact of commonly occurring tokens. It takes `raw_features` (term frequency vectors) as input and outputs weighted TF-IDF vectors in a new column named `features`.\n",
    "\n",
    "2. **Model Fitting**  \n",
    "   The IDF model is fit to the `vectorized_df` DataFrame. This computes the IDF weights based on how many documents each term appears in.\n",
    "\n",
    "3. **Transformation**  \n",
    "   The fitted model is used to transform `raw_features` into final TF-IDF vectors. The resulting DataFrame, `idf_df`, includes the TF-IDF feature column `features` alongside the article metadata and label.\n",
    "\n",
    "4. **Caching and Materialization**  \n",
    "   The transformed DataFrame is cached for efficient reuse and materialized using `.count()` to ensure the computation is executed immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview a few TF-IDF rows \n",
    "idf_df.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87adb58",
   "metadata": {},
   "source": [
    "### Pipeline Part 2: Feature Engineering\n",
    "\n",
    "#### Extract metadata features (source credibility, number of retweets, etc.) (Transformer)\n",
    "\n",
    "Here, we'll extract several metadata-based features that may help the model distinguish between real and fake news.\n",
    "\n",
    "#### Stylometric & Text-Length Features\n",
    "\n",
    "   - `text_length`: Character length of the article body.\n",
    "   - `title_length`: Character length of the article title.\n",
    "   - `exclam_count`: Number of exclamation marks in the title.\n",
    "   - `all_caps_count`: Number of fully capitalized words in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8789d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, regexp_replace, col, when\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "# text and title lengths\n",
    "news_df = news_df.withColumn(\"text_length\", length(col(\"text\")))\n",
    "news_df = news_df.withColumn(\"title_length\", length(col(\"title\")))\n",
    "\n",
    "# exclamation point feature\n",
    "news_df = news_df.withColumn(\n",
    "    \"exclam_count\", \n",
    "    length(col(\"title\")) - length(regexp_replace(col(\"title\"), \"!\", \"\"))\n",
    ")\n",
    "\n",
    "def count_all_caps_words(title):\n",
    "    if title:\n",
    "        return len([word for word in title.split() if re.fullmatch(r\"[A-Z]{2,}\", word)])\n",
    "    return 0\n",
    "\n",
    "all_caps_udf = udf(count_all_caps_words, IntegerType())\n",
    "\n",
    "news_df = news_df.withColumn(\"all_caps_count\", all_caps_udf(col(\"title\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f6d8e",
   "metadata": {},
   "source": [
    "#### Date Features\n",
    "\n",
    "- `year`, `month`, and `day` extracted from the publication date.\n",
    "- `is_2016`: Binary indicator for whether the article was published during the 2016 election year.\n",
    "- `is_missing_date`: Binary indicator for whether the date could not be parsed.\n",
    "\n",
    "   Missing date values are imputed using a placeholder (`-1`), and a separate indicator is added to allow the model to learn from missingness itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa15d143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+-----+---+\n",
      "|              date|year|month|day|\n",
      "+------------------+----+-----+---+\n",
      "|December 31, 2017 |2017|   12| 31|\n",
      "|December 29, 2017 |2017|   12| 29|\n",
      "|December 31, 2017 |2017|   12| 31|\n",
      "|December 30, 2017 |2017|   12| 30|\n",
      "|December 29, 2017 |2017|   12| 29|\n",
      "|December 29, 2017 |2017|   12| 29|\n",
      "|December 29, 2017 |2017|   12| 29|\n",
      "|December 29, 2017 |2017|   12| 29|\n",
      "|December 29, 2017 |2017|   12| 29|\n",
      "|December 28, 2017 |2017|   12| 28|\n",
      "+------------------+----+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 56:>                                                         (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    trim, col, to_date, when, year, month, dayofmonth, regexp_extract\n",
    ")\n",
    "\n",
    "# before we start parsing dates, set legacy datetime parser policy \n",
    "# so Spark doesn't yell at us \n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# clean the date string\n",
    "news_df = news_df.withColumn(\"date_clean\", trim(col(\"date\")))\n",
    "\n",
    "# try to parse two known formats\n",
    "news_df = news_df.withColumn(\n",
    "    \"parsed_date\",\n",
    "    to_date(\"date_clean\", \"MMMM d, yyyy\")\n",
    ")\n",
    "\n",
    "news_df = news_df.withColumn(\n",
    "    \"parsed_date\",\n",
    "    when(col(\"parsed_date\").isNull(), to_date(\"date_clean\", \"d-MMM-yy\"))\n",
    "    .otherwise(col(\"parsed_date\"))\n",
    ")\n",
    "\n",
    "# extract date components ( will just be null if no parsed_date)\n",
    "news_df = news_df.withColumn(\"year\", year(\"parsed_date\"))\n",
    "news_df = news_df.withColumn(\"month\", month(\"parsed_date\"))\n",
    "news_df = news_df.withColumn(\"day\", dayofmonth(\"parsed_date\"))\n",
    "\n",
    "# create Binary indicator: was this published in 2016?\n",
    "# indicative of 2016 election year\n",
    "news_df = news_df.withColumn(\"is_2016\", (col(\"year\") == 2016).cast(\"int\"))\n",
    "\n",
    "# drop intermediate columns\n",
    "news_df = news_df.drop(\"date_clean\", \"parsed_date\")\n",
    "news_df.select(\"date\", \"year\", \"month\", \"day\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f01b84",
   "metadata": {},
   "source": [
    "#### Sanity checks on engineered features above\n",
    "\n",
    "Below, it looks like we only have ~1,200 missing date-related features. This makes sense, since these represent the dates that we weren't able to parse above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "360b1587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=================================================>      (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+--------------------+----------+-----------+---------+-------------+\n",
      "|text_length_nulls|title_length_nulls|exclam_count_nulls|all_caps_count_nulls|year_nulls|month_nulls|day_nulls|is_2016_nulls|\n",
      "+-----------------+------------------+------------------+--------------------+----------+-----------+---------+-------------+\n",
      "|                0|                 0|                 0|                   0|      1270|       1270|     1270|         1270|\n",
      "+-----------------+------------------+------------------+--------------------+----------+-----------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# list engineered feature columns\n",
    "feature_cols = [\n",
    "    \"text_length\", \"title_length\", \"exclam_count\", \n",
    "    \"all_caps_count\", \"year\", \"month\", \"day\", \"is_2016\"\n",
    "]\n",
    "\n",
    "# count how many nulls in each feature column\n",
    "news_df.select([\n",
    "    F.sum(col(c).isNull().cast(\"int\")).alias(f\"{c}_nulls\")\n",
    "    for c in feature_cols\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6da831",
   "metadata": {},
   "source": [
    "#### Given this is only about 2.9% of the data, maybe we can just drop observations with these missing features.\n",
    "\n",
    "Let's do a quick sanity check and see if the observations with missing date features are inherently different from those who don't have missing date features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49090664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|missing_date|fake|count|\n",
      "+------------+----+-----+\n",
      "|       false|   0|21323|\n",
      "|       false|   1|21674|\n",
      "|        true|   0|   93|\n",
      "|        true|   1| 1177|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+----------------+----------------+------------------+\n",
      "|missing_date|text_length_avg|title_length_avg|exclam_count_avg|all_caps_count_avg|\n",
      "+------------+---------------+----------------+----------------+------------------+\n",
      "|true        |1641.62        |99.01           |0.15            |1.72              |\n",
      "|false       |2485.61        |79.36           |0.07            |1.2               |\n",
      "+------------+---------------+----------------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "news_df = news_df.withColumn(\"missing_date\", col(\"year\").isNull())\n",
    "\n",
    "news_df.groupBy(\"missing_date\", \"fake\").count().orderBy(\"missing_date\", \"fake\").show()\n",
    "\n",
    "feature_cols = [\n",
    "    \"text_length\", \"title_length\", \"exclam_count\", \"all_caps_count\",\n",
    "]\n",
    "\n",
    "summary_df = news_df.groupBy(\"missing_date\").agg(\n",
    "    *[F.round(F.avg(col_name), 2).alias(col_name + \"_avg\") for col_name in feature_cols]\n",
    ")\n",
    "\n",
    "summary_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433cf7ca",
   "metadata": {},
   "source": [
    "Above, we see that the vast majority of missing-date rows are fake news (1177 out of 1270). This is consistent with what I found earlier when I looked at missing text â€” the fake dataset is more likely to have formatting issues or missing values. So if I drop them, I'm disproportionately dropping fake news observations. \n",
    "\n",
    "The missing date articles also tend to:\n",
    "- Be shorter in body text,\n",
    "- Have longer, shoutier titles,\n",
    "\n",
    "These traits feel like they could correlate with low-effort, possibly fake content â€” which makes sense given the majority are labeled fake.\n",
    "\n",
    "This makes me worried about model bias if I dropped these observations, even if it's just 3% of the observations, as I might be dropping a unique stylometric cluster of fake articles. So, I will impute the missing date-feature data.\n",
    "\n",
    "#### Imputing missing data for `year`, `month`, `day`, `is_2016`. \n",
    "\n",
    "I will plug in `-1` for Null `year`, `month`, and `day` observations.\n",
    "\n",
    "Our `is_2016` binary indicator is tricky because a null year means we canâ€™t confidently determine whether the article was published during the election year. Therefore, I will retain `is_2016` as a 0/1 binary feature, code `is_2016` to 0 for observations with missing dates, and then introduce a new flag `is_missing_date` to explicitly mark rows with unparseable dates. This way, the model can learn patterns from the absence of date information, particularly since that absence may itself be predictive of fake content, and also interpret `is_2016 = 0` differently depending on whether the date was missing or simply not from 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6b3d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-----+\n",
      "|is_missing_date|is_2016|count|\n",
      "+---------------+-------+-----+\n",
      "|              0|      0|27302|\n",
      "|              0|      1|15695|\n",
      "|              1|      0| 1270|\n",
      "+---------------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, isnan\n",
    "\n",
    "# impute missing year, month, day with -1\n",
    "news_df = news_df.withColumn(\"year\", when(col(\"year\").isNull(), -1).otherwise(col(\"year\")))\n",
    "news_df = news_df.withColumn(\"month\", when(col(\"month\").isNull(), -1).otherwise(col(\"month\")))\n",
    "news_df = news_df.withColumn(\"day\", when(col(\"day\").isNull(), -1).otherwise(col(\"day\")))\n",
    "\n",
    "# create is_missing_date indicator\n",
    "news_df = news_df.withColumn(\"is_missing_date\", (col(\"year\") == -1).cast(\"int\"))\n",
    "\n",
    "# impute is_2016 to 0 if missing, but keep feature for learning\n",
    "news_df = news_df.withColumn(\"is_2016\", when(col(\"year\") == 2016, 1).otherwise(0))\n",
    "\n",
    "# At this point:\n",
    "# - `year`, `month`, `day` = -1 if missing\n",
    "# - `is_missing_date` = 1 if date unparseable\n",
    "# - `is_2016` = 0/1 regardless, with 0 meaning \"not 2016 or unknown\"\n",
    "\n",
    "# sanity check\n",
    "news_df.groupBy(\"is_missing_date\", \"is_2016\").count().orderBy(\"is_missing_date\", \"is_2016\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc1bf9",
   "metadata": {},
   "source": [
    "#### Join `news_df` with engineered features with TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dfb5412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_df schema:\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- fake: integer (nullable = false)\n",
      " |-- id: long (nullable = false)\n",
      " |-- text_length: integer (nullable = true)\n",
      " |-- title_length: integer (nullable = true)\n",
      " |-- exclam_count: integer (nullable = true)\n",
      " |-- all_caps_count: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- is_2016: integer (nullable = false)\n",
      " |-- missing_date: boolean (nullable = false)\n",
      " |-- is_missing_date: integer (nullable = true)\n",
      "\n",
      "idf_df schema:\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- fake: integer (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data type of 'id' in news_df\n",
    "print(\"news_df schema:\")\n",
    "news_df.printSchema()\n",
    "\n",
    "# Check data type of 'id' in idf_df\n",
    "print(\"idf_df schema:\")\n",
    "idf_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baca4925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_df count: 44267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf_df count: 44267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fe_df (joined) count: 44267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 76:=====================================================>  (23 + 1) / 24]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# merge news_df and idf_df \n",
    "fe_df = news_df.join(\n",
    "    idf_df.select(\"id\", \"features\"),\n",
    "    on=\"id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# make sure everything merged ok \n",
    "print(\"news_df count:\", news_df.count())\n",
    "print(\"idf_df count:\", idf_df.count())\n",
    "print(\"fe_df (joined) count:\", fe_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d3d0c",
   "metadata": {},
   "source": [
    "#### Make sure that there aren't nulls from a bad join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e51f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "fe_df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(f\"{c}_nulls\")\n",
    "    for c in [\"features\", \"text_length\", \"title_length\", \"is_2016\"]\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad5261",
   "metadata": {},
   "source": [
    "#### Vector Assembler\n",
    "\n",
    "This block combines all engineered features into a single feature vector that can be used for model training:\n",
    "\n",
    "1. **VectorAssembler Initialization**  \n",
    "   A `VectorAssembler` is used to merge multiple individual feature columns into one consolidated column, `final_features`. The input features include:\n",
    "   - Stylometric and structural features: `text_length`, `title_length`, `exclam_count`, `all_caps_count`\n",
    "   - Date features: `year`, `month`, `day`, `is_2016`, `is_missing_date`\n",
    "   - Text features: `features`, which contains the TF-IDF vector\n",
    "\n",
    "2. **Transformation**  \n",
    "   The assembler is applied to `fe_df`, producing a new DataFrame where each row contains:\n",
    "   - `id`: the unique identifier for the article\n",
    "   - `final_features`: a dense vector combining all numeric and text-based features\n",
    "   - `fake`: the binary label (1 = fake, 0 = real)\n",
    "\n",
    "3. **Materialization**  \n",
    "   The resulting DataFrame `final_df` is evaluated using `.count()` to force computation and ensure that all transformations have been applied successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21643129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44267"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"text_length\", \n",
    "        \"title_length\", \n",
    "        \"exclam_count\", \n",
    "        \"all_caps_count\", \n",
    "        \"year\", \n",
    "        \"month\", \n",
    "        \"day\", \n",
    "        \"is_2016\",\n",
    "        \"is_missing_date\",\n",
    "        \"features\"  # TF-IDF\n",
    "    ],\n",
    "    outputCol=\"final_features\"\n",
    ")\n",
    "\n",
    "final_df = assembler.transform(fe_df).select(\"id\", \"final_features\", \"fake\")\n",
    "final_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab353f1",
   "metadata": {},
   "source": [
    "### Pipeline Part 3: Fake News Classifier\n",
    "\n",
    "#### Train a machine learning model \n",
    "\n",
    "Note: In this project, I opted to construct the machine learning workflow manually rather than using Sparkâ€™s `Pipeline()` API. I wanted prioritize transparency and control because:\n",
    "\n",
    "* During the assignment, I ran into issues with limited cluster resources. Building the pipeline step-by-step allowed me to cache intermediate results, manage memory explicitly, and avoid recomputation across stages.\n",
    "* Additionally, I did substantial feature engineering (e.g., stylometric features, date parsing, custom imputation logic) that I couldn't manage to fit neatly into the standard transformer-estimator structure of a `Pipeline()`.\n",
    "* Finally, by separating each stage (tokenization, TF-IDF, feature engineering, etc.), I could easily inspect transformations, check for nulls, and validate my choices before training the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e97eec",
   "metadata": {},
   "source": [
    "#### Split the data into training and testing data \n",
    "\n",
    "Here I use a 70/30 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ce644f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.count(): 31086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:==============================================>         (20 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.count(): 13181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#split\n",
    "train_df, test_df = final_df.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "#cache\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "# sanity check \n",
    "print(\"train_df.count():\", train_df.count())\n",
    "print(\"test_df.count():\", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876e134",
   "metadata": {},
   "source": [
    "Breakdown of the datasets by real vs. fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4568a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|fake|count|\n",
      "+----+-----+\n",
      "|   0|15011|\n",
      "|   1|16075|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|fake|count|\n",
      "+----+-----+\n",
      "|   0| 6405|\n",
      "|   1| 6776|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy(\"fake\").count().show()\n",
    "test_df.groupBy(\"fake\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b6de3",
   "metadata": {},
   "source": [
    "#### Start by fitting a logistic regression model, then predict the test data based on the model.\n",
    "\n",
    "This section defines, trains, and applies a logistic regression classifier for fake news detection. I decided to start with a logistic regression model, since it is a natural starting point for binary classification problems (i.e., is the article fake news or not?). \n",
    "\n",
    "Logistic regression also provides direct insight into how individual features contribute to the predicted probability of a news article being fake, which is useful for understanding the influence of both textual and metadata features.\n",
    "\n",
    "1. **Model Initialization**  \n",
    "   A `LogisticRegression` model is created using:\n",
    "   - `featuresCol=\"final_features\"`: the vector of assembled input features\n",
    "   - `labelCol=\"fake\"`: the binary target column (1 = fake, 0 = real)\n",
    "   - `maxIter=10`: the maximum number of iterations for optimization\n",
    "   - `regParam=0.01`: L2 regularization parameter to help prevent overfitting\n",
    "\n",
    "2. **Model Fitting**  \n",
    "   The model is trained on the `train_df` dataset, which contains feature vectors and corresponding labels.\n",
    "\n",
    "3. **Prediction**  \n",
    "   The trained model is applied to the `test_df` dataset using `.transform()`, producing a new DataFrame containing:\n",
    "   - `fake`: the actual label\n",
    "   - `prediction`: the predicted label from the model\n",
    "   - `probability`: the model's predicted probability for each class (real or fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72a0ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------------------+\n",
      "|fake|prediction|         probability|\n",
      "+----+----------+--------------------+\n",
      "|   0|       0.0|[0.95732527961460...|\n",
      "|   0|       0.0|[0.98965375375120...|\n",
      "|   0|       0.0|[0.94115768157027...|\n",
      "|   0|       0.0|[0.95016892309915...|\n",
      "|   0|       0.0|[0.91634009973255...|\n",
      "+----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "\n",
    "lr = cl.LogisticRegression(  # logistic regression model \n",
    "    maxIter = 10,                 \n",
    "    regParam = 0.01,               \n",
    "    featuresCol = \"final_features\",\n",
    "    labelCol = \"fake\")            # label column name.\n",
    "\n",
    "# fit the model on the training data\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"fake\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fbd8b",
   "metadata": {},
   "source": [
    "### Pipeline Part 4: Evaluation\n",
    "\n",
    "#### Precision, recall, and AUC-ROC for classification performance\n",
    "\n",
    "This section evaluates the performance of the trained logistic regression model.\n",
    "\n",
    "#### 1. Binary Classification Metrics (Threshold-Independent)\n",
    "These metrics evaluate the model's ability to rank predictions, regardless of the specific classification threshold:\n",
    "\n",
    "- **Area Under ROC Curve (AUC-ROC)**: Measures how well the model distinguishes between the two classes (fake vs. real) across all thresholds. A value close to 1.0 indicates excellent separability.\n",
    "- **Area Under Precision-Recall Curve (AUC-PR)**: Focuses on the model's ability to capture the positive class (fake = 1), particularly useful in cases of class imbalance.\n",
    "\n",
    "#### 2. Classification Metrics at Default Threshold (Threshold = 0.5)\n",
    "These metrics are calculated based on the model's predicted class labels after thresholding:\n",
    "\n",
    "- **Accuracy**: Proportion of all predictions that are correct.\n",
    "- **Precision (fake = 1)**: Among all articles predicted as fake, the proportion that were actually fake.\n",
    "- **Recall (fake = 1)**: Among all actual fake articles, the proportion that the model correctly identified.\n",
    "\n",
    "These metrics provide insight into how well the model performs in practice when making binary decisions.\n",
    "\n",
    "#### 3. Confusion Matrix\n",
    "A confusion matrix is printed by grouping by the true label (`fake`) and the predicted label. This gives a breakdown of:\n",
    "\n",
    "- True Positives (fake=1, predicted=1)\n",
    "- False Positives (fake=0, predicted=1)\n",
    "- True Negatives (fake=0, predicted=0)\n",
    "- False Negatives (fake=1, predicted=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77533323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              Metric|               Value|\n",
      "+--------------------+--------------------+\n",
      "|        AUC-ROC (LR)|  0.9988484982124539|\n",
      "|         AUC-PR (LR)|  0.9987513830823052|\n",
      "|       Accuracy (LR)|  0.9888475836431226|\n",
      "|Precision (fake=1...|  0.9933025747879148|\n",
      "|Recall (fake=1) (LR)|  0.9849468713105076|\n",
      "|Confusion Matrix ...|fake: 0, predicti...|\n",
      "|Confusion Matrix ...|fake: 0, predicti...|\n",
      "|Confusion Matrix ...|fake: 1, predicti...|\n",
      "|Confusion Matrix ...|fake: 1, predicti...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 165:================================================>        (6 + 1) / 7]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# === Binary Classification Metrics ===\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"probability\",\n",
    "    labelCol=\"fake\"\n",
    ")\n",
    "\n",
    "# AUC-ROC\n",
    "binary_evaluator.setMetricName(\"areaUnderROC\")\n",
    "auc_roc = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "# AUC-PR\n",
    "binary_evaluator.setMetricName(\"areaUnderPR\")\n",
    "auc_pr = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "# Store binary classification metrics in results\n",
    "results.append((\"AUC-ROC (LR)\", auc_roc))\n",
    "results.append((\"AUC-PR (LR)\", auc_pr))\n",
    "\n",
    "# === Accuracy, Precision, Recall (fake=1) ===\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"fake\"\n",
    ")\n",
    "\n",
    "accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "precision_fake = multi_evaluator.setMetricName(\"precisionByLabel\").evaluate(predictions, {multi_evaluator.metricLabel: 1.0})\n",
    "recall_fake = multi_evaluator.setMetricName(\"recallByLabel\").evaluate(predictions, {multi_evaluator.metricLabel: 1.0})\n",
    "\n",
    "# Store classification metrics in results\n",
    "results.append((\"Accuracy (LR)\", accuracy))\n",
    "results.append((\"Precision (fake=1) (LR)\", precision_fake))\n",
    "results.append((\"Recall (fake=1) (LR)\", recall_fake))\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "confusion_df = predictions.groupBy(\"fake\", \"prediction\").count().orderBy(\"fake\", \"prediction\")\n",
    "\n",
    "# Collect confusion matrix data into a list\n",
    "confusion_data = confusion_df.collect()\n",
    "for row in confusion_data:\n",
    "    results.append((\"Confusion Matrix (LR)\", f\"fake: {row['fake']}, prediction: {row['prediction']}, count: {row['count']}\"))\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = spark.createDataFrame(results, [\"Metric\", \"Value\"])\n",
    "\n",
    "# Show the results DataFrame\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363ec47",
   "metadata": {},
   "source": [
    "Above, we see the logistic regression model performed extremely well on the test set. Both the Area Under the ROC Curve (AUC-ROC) and Area Under the Precision-Recall Curve (AUC-PR) had high values of 0.9988 and 0.9987 respectively. This suggests that the model is highly effective at distinguishing between real and fake news articles across a range of classification thresholds.\n",
    "\n",
    "When evaluating the model using standard classification metrics, we see similarly strong results:\n",
    "- **Accuracy**: 98.9% of predictions were correct overall.\n",
    "- **Precision (fake = 1)**: 99.3% of the articles predicted as fake were actually fake.\n",
    "- **Recall (fake = 1)**: 98.5% of all fake articles in the dataset were correctly identified by the model.\n",
    "\n",
    "The confusion matrix helps break this down:\n",
    "- The model correctly predicted 6,675 out of 6,776 fake articles.\n",
    "- It only misclassified 101 fake articles as real (false negatives).\n",
    "- For real articles, it correctly identified 6,360 out of 6,405, with just 45 false positives.\n",
    "\n",
    "Overall, the model not only achieves high accuracy, but also balances precision and recall very well. While all evaluation metrics indicate strong model performance, recall (for fake = 1) is particularly important in a real-world context. In fake news detection, we want to identify as many fake articles as possible and make sure we aren't interpreting fake news and \"real\". A high recall means the model is effectively catching most fake content, minimizing the risk of misinformation slipping through undetected. This is especially critical in settings like content moderation or public information platforms, where missing fake news can have real consequences.\n",
    "\n",
    "\n",
    "#### Print the model's intercept and coefficients for the 9 engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c476d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -2.1776\n",
      "Number of coefficients: 5009\n",
      "\n",
      "First 9 coefficients (engineered features):\n",
      "text_length       => +0.0000\n",
      "title_length      => +0.0303\n",
      "exclam_count      => +1.4708\n",
      "all_caps_count    => +0.4339\n",
      "year              => -0.0002\n",
      "month             => -0.0631\n",
      "day               => -0.0000\n",
      "is_2016           => +0.6725\n",
      "is_missing_date   => +0.4940\n"
     ]
    }
   ],
   "source": [
    "# Define feature names (engineered features only â€” TF-IDF comes after)\n",
    "engineered_features = [\n",
    "    \"text_length\", \n",
    "    \"title_length\", \n",
    "    \"exclam_count\", \n",
    "    \"all_caps_count\", \n",
    "    \"year\", \n",
    "    \"month\", \n",
    "    \"day\", \n",
    "    \"is_2016\",\n",
    "    \"is_missing_date\"\n",
    "]\n",
    "\n",
    "print(f\"Intercept: {lr_model.intercept:.4f}\")\n",
    "print(f\"Number of coefficients: {len(lr_model.coefficients)}\")\n",
    "print(\"\\nFirst 9 coefficients (engineered features):\")\n",
    "\n",
    "for i, feat in enumerate(engineered_features):\n",
    "    coeff = lr_model.coefficients[i]\n",
    "    print(f\"{feat:17s} => {coeff:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2baef0d",
   "metadata": {},
   "source": [
    "The intercept of the logistic regression model is -2.1762, which basically means that in the absence of any strong feature signals (i.e. all features = 0), the model leans toward predicting an article as real (label 0). Thatâ€™s expected, given that most articles in the dataset are real unless something in the content tips it the other way.\n",
    "\n",
    "Looking at the engineered features, a few stand out:\n",
    "\n",
    "- **Exclamation count (+1.47)** and **ALL CAPS word count (+0.43)** have large positive coefficients, which makes intuitive sense â€” these are pretty classic indicators of sensationalized or less professional writing.\n",
    "- **is_2016 (+0.67)** and **is_missing_date (+0.50)** are also strongly positive. So articles published in 2016 (election year) or articles with messy/missing dates are more likely to be flagged as fake, which aligns with earlier patterns I noticed in the dataset.\n",
    "- Length-based features like `text_length` and `title_length` donâ€™t play much of a role â€” their coefficients are close to zero.\n",
    "\n",
    "#### Print the coefficients for the TF-IDF features (5000 of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3252450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 TF-IDF features by absolute coefficient value:\n",
      "\n",
      "washington           => -0.3075\n",
      "wednesday            => -0.2566\n",
      "nov                  => -0.2198\n",
      "tuesday              => -0.2164\n",
      "somodevilla          => +0.2122\n",
      "thursday             => -0.2096\n",
      "corrects             => -0.2054\n",
      "representatives      => -0.1997\n",
      "youtu                => +0.1962\n",
      "friday               => -0.1825\n"
     ]
    }
   ],
   "source": [
    "# Number of engineered features already accounted for\n",
    "offset = len(engineered_features)\n",
    "\n",
    "# Extract only the TF-IDF portion of the coefficients\n",
    "tfidf_coeffs = lr_model.coefficients[offset:]\n",
    "\n",
    "# Zip with vocabulary from CountVectorizer model\n",
    "vocab = vectorizer_model.vocabulary  # already sorted by frequency\n",
    "tfidf_feature_info = list(zip(vocab, tfidf_coeffs))\n",
    "\n",
    "# Sort by absolute coefficient value, descending\n",
    "tfidf_feature_info_sorted = sorted(tfidf_feature_info, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Display top 10 TF-IDF features with the strongest coefficients\n",
    "print(\"\\nTop 10 TF-IDF features by absolute coefficient value:\\n\")\n",
    "for token, coeff in tfidf_feature_info_sorted[:10]:\n",
    "    print(f\"{token:20s} => {coeff:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d2f08",
   "metadata": {},
   "source": [
    "On the TF-IDF side, the model seems to pick up on contextual patterns:\n",
    "\n",
    "- Words like **\"washington\"**, **\"wednesday\"**, and other weekday/date terms have negative coefficients, suggesting that more formal or structured reporting (common in real news) leans that direction.\n",
    "- On the other hand, terms like **\"somodevilla\"** (probably from image captions), **\"youtu\"**, and other more casual or clickbaity artifacts push predictions toward fake. I might consider adding these to my list of stopwords in future model tuning.\n",
    "- It's also interesting to see **\"corrects\"** and **\"representatives\"** come in with negative weights â€” potentially reflecting real news correcting the record or covering government procedures.\n",
    "\n",
    "Overall, the model seems to be relying both on stylometric features and lexical signals to separate professional reporting from fake or less credible articles â€” and the signals it's using mostly make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990b852",
   "metadata": {},
   "source": [
    "### Pipeline Part 5: Tuning and fitting more sophisticated model\n",
    "\n",
    "In this section, I perform hyperparameter tuning that cycles and testing different values of `maxIter` and `regParam` in order to identify the best model parameters. Because the intial logistic regression model is already very precise, I don't think this tuning will change much.\n",
    "\n",
    "Next, I evaluate a more sophisticated Random Forest classifier. This model is worth considering for binary classification tasks like fake news detection because it can capture complex, nonlinear patterns in the data and handle interactions between features without needing explicit transformations. Random forests also tend to be less sensitive to outliers and may offer improved performance.\n",
    "\n",
    "#### Hyperparameter tuning\n",
    "\n",
    "In this block, I perform hyperparameter tuning using cross-validation to optimize the logistic regression model.\n",
    "\n",
    "I used a grid of parameters using `ParamGridBuilder`: \n",
    "   - `maxIter` (number of training iterations): tested at 2, 10, and 50  \n",
    "   - `regParam` (regularization strength): tested at 0.01, 0.05, and 0.3\n",
    "\n",
    "I then use `BinaryClassificationEvaluator` is used to compare models, scoring them by Area Under the ROC Curve (AUC).\n",
    "\n",
    "This process confirms whether tuning the logistic regression parameters yields any significant improvement â€” though given the already strong baseline performance, major gains arenâ€™t expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "937e8e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Params:\n",
      "  maxIter: 50\n",
      "  regParam: 0.01\n",
      "Best Model AUC on Test: 0.9987938787491695\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# define the base logistic regression model\n",
    "logistic = LogisticRegression(\n",
    "    featuresCol=\"final_features\",\n",
    "    labelCol=\"fake\"\n",
    ")\n",
    "\n",
    "# build  param grid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(logistic.maxIter, [2, 10, 50])  \n",
    "    .addGrid(logistic.regParam, [0.01, 0.05, 0.3]) \n",
    "    .build()\n",
    ")\n",
    "\n",
    "# define the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"probability\",\n",
    "    labelCol=\"fake\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# set up CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=logistic,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2,  \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# fit on the unshuffled training set (shuffle can be re-added later)\n",
    "cvModel = cv.fit(train_df)\n",
    "\n",
    "# evaluate best model on the test set\n",
    "bestModel = cvModel.bestModel\n",
    "print(\"Best Model Params for LR Model:\")\n",
    "print(\"  maxIter:\", bestModel._java_obj.getMaxIter())\n",
    "print(\"  regParam:\", bestModel._java_obj.getRegParam())\n",
    "\n",
    "predictions = bestModel.transform(test_df)\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Best Model AUC on Test:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a18e966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------------------------------+\n",
      "|Metric                       |Value                                |\n",
      "+-----------------------------+-------------------------------------+\n",
      "|AUC-ROC (LR)                 |0.9988484982124539                   |\n",
      "|AUC-PR (LR)                  |0.9987513830823052                   |\n",
      "|Accuracy (LR)                |0.9888475836431226                   |\n",
      "|Precision (fake=1) (LR)      |0.9933025747879148                   |\n",
      "|Recall (fake=1) (LR)         |0.9849468713105076                   |\n",
      "|Confusion Matrix (LR)        |fake: 0, prediction: 0.0, count: 6360|\n",
      "|Confusion Matrix (LR)        |fake: 0, prediction: 1.0, count: 45  |\n",
      "|Confusion Matrix (LR)        |fake: 1, prediction: 0.0, count: 102 |\n",
      "|Confusion Matrix (LR)        |fake: 1, prediction: 1.0, count: 6674|\n",
      "|AUC-ROC (LR Tuned)           |0.9987926921208802                   |\n",
      "|AUC-PR (LR Tuned)            |0.9986753314595586                   |\n",
      "|Accuracy (LR Tuned)          |0.9895303846445641                   |\n",
      "|Precision (fake=1) (LR Tuned)|0.9919952564482657                   |\n",
      "|Recall (fake=1) (LR Tuned)   |0.987603305785124                    |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 0, prediction: 0.0, count: 6351|\n",
      "|Confusion Matrix (LR Tuned)  |fake: 0, prediction: 1.0, count: 54  |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 1, prediction: 0.0, count: 84  |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 1, prediction: 1.0, count: 6692|\n",
      "+-----------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "results_tuned = []\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"probability\",\n",
    "    labelCol=\"fake\"\n",
    ")\n",
    "\n",
    "# AUC-ROC\n",
    "binary_evaluator.setMetricName(\"areaUnderROC\")\n",
    "auc_roc_tuned = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "# AUC-PR\n",
    "binary_evaluator.setMetricName(\"areaUnderPR\")\n",
    "auc_pr_tuned = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "results_tuned.append((\"AUC-ROC (LR Tuned)\", auc_roc_tuned))\n",
    "results_tuned.append((\"AUC-PR (LR Tuned)\", auc_pr_tuned))\n",
    "\n",
    "# Accuracy, Precision, Recall \n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"fake\"\n",
    ")\n",
    "\n",
    "accuracy_tuned = multi_evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "precision_fake_tuned = multi_evaluator.setMetricName(\"precisionByLabel\").evaluate(predictions, {multi_evaluator.metricLabel: 1.0})\n",
    "recall_fake_tuned = multi_evaluator.setMetricName(\"recallByLabel\").evaluate(predictions, {multi_evaluator.metricLabel: 1.0})\n",
    "\n",
    "results_tuned.append((\"Accuracy (LR Tuned)\", accuracy_tuned))\n",
    "results_tuned.append((\"Precision (fake=1) (LR Tuned)\", precision_fake_tuned))\n",
    "results_tuned.append((\"Recall (fake=1) (LR Tuned)\", recall_fake_tuned))\n",
    "\n",
    "#  Confusion Matrix \n",
    "confusion_df_tuned = predictions.groupBy(\"fake\", \"prediction\").count().orderBy(\"fake\", \"prediction\")\n",
    "confusion_data_tuned = confusion_df_tuned.collect()\n",
    "for row in confusion_data_tuned:\n",
    "    results_tuned.append((\"Confusion Matrix (LR Tuned)\", f\"fake: {row['fake']}, prediction: {row['prediction']}, count: {row['count']}\"))\n",
    "\n",
    "#  Convert new results to a DataFrame \n",
    "results_df_tuned = spark.createDataFrame(results_tuned, [\"Metric\", \"Value\"])\n",
    "\n",
    "#  Append to the original results DataFrame \n",
    "results_df = results_df.unionByName(results_df_tuned)\n",
    "\n",
    "# Show the updated results\n",
    "results_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d81b5f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.clearCache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f153d",
   "metadata": {},
   "source": [
    "After hypertuning  `maxIter` and `regParam`, the best logistic regression model had:\n",
    "- **maxIter = 50**\n",
    "- **regParam = 0.01**\n",
    "\n",
    "This model achieved an AUC of 0.99879 on the test set, which is essentially identical to the original modelâ€™s AUC of 0.9988. This confirms my initial expectation â€” that the base model was already performing near the ceiling in terms of ROC performance, and tuning didnâ€™t meaningfully change the outcome.\n",
    "\n",
    "#### Fitting a Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f4c3834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 14:10:05 WARN DAGScheduler: Broadcasting large task binary with size 1059.2 KiB\n",
      "25/04/11 14:10:05 WARN DAGScheduler: Broadcasting large task binary with size 1212.8 KiB\n",
      "25/04/11 14:10:06 WARN DAGScheduler: Broadcasting large task binary with size 1432.0 KiB\n",
      "25/04/11 14:10:07 WARN DAGScheduler: Broadcasting large task binary with size 1723.8 KiB\n",
      "25/04/11 14:10:08 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/04/11 14:10:09 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/04/11 14:10:11 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/04/11 14:10:24 WARN DAGScheduler: Broadcasting large task binary with size 1863.2 KiB\n",
      "25/04/11 14:10:32 WARN DAGScheduler: Broadcasting large task binary with size 1863.2 KiB\n",
      "25/04/11 14:10:40 WARN DAGScheduler: Broadcasting large task binary with size 1875.6 KiB\n",
      "25/04/11 14:10:47 WARN DAGScheduler: Broadcasting large task binary with size 1875.6 KiB\n",
      "25/04/11 14:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1875.6 KiB\n",
      "25/04/11 14:11:00 WARN DAGScheduler: Broadcasting large task binary with size 1870.6 KiB\n",
      "25/04/11 14:11:03 WARN DAGScheduler: Broadcasting large task binary with size 1836.3 KiB\n",
      "25/04/11 14:11:03 WARN DAGScheduler: Broadcasting large task binary with size 1836.2 KiB\n",
      "25/04/11 14:11:04 WARN DAGScheduler: Broadcasting large task binary with size 1835.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------------------------------+\n",
      "|Metric                       |Value                                |\n",
      "+-----------------------------+-------------------------------------+\n",
      "|AUC-ROC (LR)                 |0.9988484982124539                   |\n",
      "|AUC-PR (LR)                  |0.9987513830823052                   |\n",
      "|Accuracy (LR)                |0.9888475836431226                   |\n",
      "|Precision (fake=1) (LR)      |0.9933025747879148                   |\n",
      "|Recall (fake=1) (LR)         |0.9849468713105076                   |\n",
      "|Confusion Matrix (LR)        |fake: 0, prediction: 0.0, count: 6360|\n",
      "|Confusion Matrix (LR)        |fake: 0, prediction: 1.0, count: 45  |\n",
      "|Confusion Matrix (LR)        |fake: 1, prediction: 0.0, count: 102 |\n",
      "|Confusion Matrix (LR)        |fake: 1, prediction: 1.0, count: 6674|\n",
      "|AUC-ROC (LR Tuned)           |0.9987926921208802                   |\n",
      "|AUC-PR (LR Tuned)            |0.9986753314595586                   |\n",
      "|Accuracy (LR Tuned)          |0.9895303846445641                   |\n",
      "|Precision (fake=1) (LR Tuned)|0.9919952564482657                   |\n",
      "|Recall (fake=1) (LR Tuned)   |0.987603305785124                    |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 0, prediction: 0.0, count: 6351|\n",
      "|Confusion Matrix (LR Tuned)  |fake: 0, prediction: 1.0, count: 54  |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 1, prediction: 0.0, count: 84  |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 1, prediction: 1.0, count: 6692|\n",
      "|AUC-ROC (RF)                 |0.9957087719249738                   |\n",
      "|AUC-PR (RF)                  |0.9961013085833911                   |\n",
      "+-----------------------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Feature Importances (top 20 shown):\n",
      " Feature index 3: importance = 0.0991\n",
      " Feature index 1: importance = 0.0961\n",
      " Feature index 5: importance = 0.0351\n",
      " Feature index 4: importance = 0.0303\n",
      " Feature index 171: importance = 0.0267\n",
      " Feature index 111: importance = 0.0251\n",
      " Feature index 7: importance = 0.0240\n",
      " Feature index 854: importance = 0.0183\n",
      " Feature index 133: importance = 0.0161\n",
      " Feature index 34: importance = 0.0149\n",
      " Feature index 390: importance = 0.0141\n",
      " Feature index 2: importance = 0.0138\n",
      " Feature index 97: importance = 0.0127\n",
      " Feature index 15: importance = 0.0114\n",
      " Feature index 30: importance = 0.0111\n",
      " Feature index 33: importance = 0.0110\n",
      " Feature index 88: importance = 0.0108\n",
      " Feature index 96: importance = 0.0100\n",
      " Feature index 54: importance = 0.0091\n",
      " Feature index 859: importance = 0.0090\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# define RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"final_features\",\n",
    "    labelCol=\"fake\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# generate predictions\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "# evaluate binary metrics\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    labelCol=\"fake\"\n",
    ")\n",
    "\n",
    "binary_evaluator.setMetricName(\"areaUnderROC\")\n",
    "auc_rf = binary_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "binary_evaluator.setMetricName(\"areaUnderPR\")\n",
    "pr_rf = binary_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# evaluate multiclass metrics\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"fake\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "accuracy_rf = multi_evaluator.setMetricName(\"accuracy\").evaluate(rf_predictions)\n",
    "precision_rf = multi_evaluator.setMetricName(\"precisionByLabel\").evaluate(rf_predictions, {multi_evaluator.metricLabel: 1.0})\n",
    "recall_rf = multi_evaluator.setMetricName(\"recallByLabel\").evaluate(rf_predictions, {multi_evaluator.metricLabel: 1.0})\n",
    "\n",
    "# collect confusion matrix\n",
    "confusion_df_rf = rf_predictions.groupBy(\"fake\", \"prediction\").count().orderBy(\"fake\", \"prediction\")\n",
    "confusion_data_rf = confusion_df_rf.collect()\n",
    "\n",
    "#append all results to results_df\n",
    "results_rf = [\n",
    "    (\"AUC-ROC (RF)\", auc_rf),\n",
    "    (\"AUC-PR (RF)\", pr_rf),\n",
    "    (\"Accuracy (RF)\", accuracy_rf),\n",
    "    (\"Precision (fake=1) (RF)\", precision_rf),\n",
    "    (\"Recall (fake=1) (RF)\", recall_rf)\n",
    "]\n",
    "\n",
    "# Add confusion matrix entries\n",
    "for row in confusion_data_rf:\n",
    "    results_rf.append((\"Confusion Matrix (RF)\", f\"fake: {row['fake']}, prediction: {row['prediction']}, count: {row['count']}\"))\n",
    "\n",
    "# Convert and append\n",
    "results_df_rf = spark.createDataFrame(results_rf, [\"Metric\", \"Value\"])\n",
    "results_df = results_df.unionByName(results_df_rf)\n",
    "\n",
    "# Show results\n",
    "results_df.show(30,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5c9dea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature importances for engineered features:\n",
      "text_length        => 0.0077\n",
      "title_length       => 0.0961\n",
      "exclam_count       => 0.0138\n",
      "all_caps_count     => 0.0991\n",
      "year               => 0.0303\n",
      "month              => 0.0351\n",
      "day                => 0.0038\n",
      "is_2016            => 0.0240\n",
      "is_missing_date    => 0.0010\n",
      "\n",
      "Top 10 TF-IDF features by importance:\n",
      "watch                => 0.0267\n",
      "video                => 0.0251\n",
      "wire                 => 0.0183\n",
      "com                  => 0.0161\n",
      "hillary              => 0.0149\n",
      "gop                  => 0.0141\n",
      "minister             => 0.0127\n",
      "government           => 0.0114\n",
      "washington           => 0.0111\n",
      "us                   => 0.0110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "engineered_feature_names = [\n",
    "    \"text_length\", \"title_length\", \"exclam_count\", \"all_caps_count\",\n",
    "    \"year\", \"month\", \"day\", \"is_2016\", \"is_missing_date\"\n",
    "]\n",
    "num_engineered = len(engineered_feature_names)\n",
    "\n",
    "# combine feature names\n",
    "feature_names = engineered_feature_names + vocab  # full feature list\n",
    "\n",
    "# get feature importances\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# engineered features (first N)\n",
    "print(\"\\nFeature importances for engineered features:\")\n",
    "for i in range(num_engineered):\n",
    "    print(f\"{feature_names[i]:<18} => {importances[i]:.4f}\")\n",
    "\n",
    "# top 10 TF-IDF features by importance\n",
    "tfidf_indices = range(num_engineered, len(feature_names))\n",
    "top_tfidf = sorted(tfidf_indices, key=lambda i: importances[i], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF features by importance:\")\n",
    "for i in top_tfidf:\n",
    "    print(f\"{feature_names[i]:<20} => {importances[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7313426",
   "metadata": {},
   "source": [
    "The Random Forest model gives us a different way to understand which features contributed most to the classification.\n",
    "\n",
    "Among the engineered features, `all_caps_count` and `title_length` stood out with the highest importance, suggesting that stylistic elements like the use of all caps and longer titles were particularly useful in identifying fake news. `exclam_count` and `text_length` also played a role, though to a lesser extent. Interestingly, temporal features like `year`, `month`, and `is_2016` also carried some weight, indicating that when the content was published had some predictive value.\n",
    "\n",
    "Looking at the top TF-IDF features, the most important terms included `watch`, `video`, `wire`, and `com`, which might reflect the kind of clickbait or media-heavy language often used in fake posts. Terms like `hillary`, `gop`, `government`, and `washington` also appeared, which lines up with the political focus thatâ€™s common in misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b31d0",
   "metadata": {},
   "source": [
    "### Model Comparison and Interpretation\n",
    "\n",
    "I trained three models to classify fake news: a baseline logistic regression (LR), a tuned logistic regression (LR Tuned), and a random forest (RF). All models used the same feature set.\n",
    "\n",
    "#### Performance Summary\n",
    "\n",
    "If the goal is to identify fake news posts â€” especially to catch as many as possible â€” then **precision and recall for the fake class (label=1)** are key. Here's how the models compare:\n",
    "\n",
    "- **Logistic Regression (Baseline)**\n",
    "  - AUC-ROC: 0.9988\n",
    "  - Precision (fake=1): 0.9933\n",
    "  - Recall (fake=1): 0.9849\n",
    "  - Accuracy: 0.9888\n",
    "\n",
    "- **Logistic Regression (Tuned)**\n",
    "  - AUC-ROC: 0.9988\n",
    "  - Precision (fake=1): 0.9920\n",
    "  - Recall (fake=1): 0.9876\n",
    "  - Accuracy: 0.9895\n",
    "\n",
    "- **Random Forest**\n",
    "  - AUC-ROC: 0.9957\n",
    "  - Precision (fake=1): 0.9969\n",
    "  - Recall (fake=1): 0.8597\n",
    "  - Accuracy: 0.9265\n",
    "\n",
    "While the random forest had the highest precision, it suffered in recall â€” missing more fake articles than the logistic models. On the other hand, the logistic regression models offered a better balance between precision and recall. Since I value identifying fake news accurately and minimizing false negatives, logistic regression (tuned or untuned) is the stronger choice in this context.\n",
    "\n",
    "#### Most Influential Features\n",
    "\n",
    "Both models shed light on what signals are most useful for detecting fake news:\n",
    "\n",
    "- **Engineered Metadata Features**\n",
    "  - In both models, **`all_caps_count`**, **`exclam_count`**, and **`title_length`** had high influence. These stylistic choices are common in sensational or misleading content.\n",
    "  - The logistic regression model also gave significant weight to **`is_2016`** and **`is_missing_date`**, suggesting temporal cues and metadata inconsistencies were meaningful indicators.\n",
    "\n",
    "- **Top TF-IDF Words**\n",
    "  - The logistic regression model emphasized politically and temporally loaded terms like:\n",
    "    ```\n",
    "    washington, wednesday, nov, tuesday, somodevilla, thursday, corrects, representatives, youtu, friday\n",
    "    ```\n",
    "    These words â€” especially day-of-week and political proper nouns â€” likely reflect the structure and topics of fake articles during the dataset's peak periods.\n",
    "  \n",
    "  - The random forest model gave importance to slightly different terms:\n",
    "    ```\n",
    "    watch, video, wire, com, hillary, gop, minister, government, washington, us\n",
    "    ```\n",
    "    This includes action-oriented and political terms, with a heavier emphasis on media consumption language (`watch`, `video`, `com`), which aligns with fake news strategies that rely on viral media content.\n",
    "\n",
    "#### Final Thoughts\n",
    "\n",
    "Despite Random Forestâ€™s strong precision, its lower recall makes it less suited for use cases where catching fake news is critical. Logistic regression â€” even without tuning â€” performed exceptionally well across all metrics, making it the preferred model. It also offers greater interpretability, which is useful for understanding what drives the classification and how misinformation is structured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8563f0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------------------------------+\n",
      "|Metric                       |Value                                |\n",
      "+-----------------------------+-------------------------------------+\n",
      "|AUC-ROC (LR)                 |0.9988484982124539                   |\n",
      "|AUC-PR (LR)                  |0.9987513830823052                   |\n",
      "|Accuracy (LR)                |0.9888475836431226                   |\n",
      "|Precision (fake=1) (LR)      |0.9933025747879148                   |\n",
      "|Recall (fake=1) (LR)         |0.9849468713105076                   |\n",
      "|Confusion Matrix (LR)        |fake: 0, prediction: 0.0, count: 6360|\n",
      "|Confusion Matrix (LR)        |fake: 0, prediction: 1.0, count: 45  |\n",
      "|Confusion Matrix (LR)        |fake: 1, prediction: 0.0, count: 102 |\n",
      "|Confusion Matrix (LR)        |fake: 1, prediction: 1.0, count: 6674|\n",
      "|AUC-ROC (LR Tuned)           |0.9987926921208802                   |\n",
      "|AUC-PR (LR Tuned)            |0.9986753314595586                   |\n",
      "|Accuracy (LR Tuned)          |0.9895303846445641                   |\n",
      "|Precision (fake=1) (LR Tuned)|0.9919952564482657                   |\n",
      "|Recall (fake=1) (LR Tuned)   |0.987603305785124                    |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 0, prediction: 0.0, count: 6351|\n",
      "|Confusion Matrix (LR Tuned)  |fake: 0, prediction: 1.0, count: 54  |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 1, prediction: 0.0, count: 84  |\n",
      "|Confusion Matrix (LR Tuned)  |fake: 1, prediction: 1.0, count: 6692|\n",
      "|AUC-ROC (RF)                 |0.9957087719249738                   |\n",
      "|AUC-PR (RF)                  |0.9961013085833911                   |\n",
      "|Accuracy (RF)                |0.9264850921781352                   |\n",
      "|Precision (fake=1) (RF)      |0.9969193907239432                   |\n",
      "|Recall (fake=1) (RF)         |0.8596517119244392                   |\n",
      "|Confusion Matrix (RF)        |fake: 0, prediction: 0.0, count: 6387|\n",
      "|Confusion Matrix (RF)        |fake: 0, prediction: 1.0, count: 18  |\n",
      "|Confusion Matrix (RF)        |fake: 1, prediction: 0.0, count: 951 |\n",
      "|Confusion Matrix (RF)        |fake: 1, prediction: 1.0, count: 5825|\n",
      "+-----------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df.show(30,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
